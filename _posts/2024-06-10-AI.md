---
layout: post
title: Intuition / Thinking point for AI explained   
date : 2024-06-10
---

[KL-Divergence](https://www.youtube.com/watch?v=tXE23653JrU)
[flow-matching](https://www.youtube.com/watch?v=7cMzfkWFWhI)

Generative models are very good / best in class approximators of a complex probabilistic equation/distribution that we most of the times haveÂ no idea of !! 


Images modality : All images in the world are from a very complex distribution of pixels that gives direction , based on the prompts , and are dependent of what the user wants , and as it very complex to model that distribution we rely on NN to predict it , hence we have diffusion models

Text modality : Same as above 

What about Classification models ?   
There also we have true distribution and we need to minimize the output distribution and to minimize that distribution we use cross entropy as its a the term in KLD that is actually dependent on the model params !!

`KLD = entropy(P,Q) - entropy(P)` , deviation between true and unknown distribution !!  

`entropy(P,Q) = cross_entropy(P,Q)`


So this way we calculate the loss !! and update the parameters / weights of the model ( aka the complex equation estimator )  


Diffusion models they go a step ahead, they start with a gaussian distribution and try to approximate the data distribution


